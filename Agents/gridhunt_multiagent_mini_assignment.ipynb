{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents: Reflex-Based Agents for GridHunt (Multi-agent Environment)\n",
    "\n",
    "Student Name: [Add your name]\n",
    "\n",
    "I have used the following AI tools: [list tools]\n",
    "\n",
    "I understand that my submission needs to be my own work: [your initials]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "* Apply core AI concepts by implementing the agent function for a simple and model-based reflex agents that respond to environmental percepts.\n",
    "* Practice how the environment and the agent function interact.\n",
    "* Analyze agent performance through controlled experiments across different environment configurations.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undergrads 10\n",
    "\n",
    "Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the completely rendered notebook.\n",
    "### AI Use\n",
    "\n",
    "Here are some guidelines that will make it easier for you:\n",
    "\n",
    "* __Don't:__ Rely on AI auto completion. You will waste a lot of time trying to figure out how the suggested code relates to what we do in class. Turn off AI code completion (e.g., Copilot) in your IDE.\n",
    "* __Don't:__ Do not submit code/text that you do not understand or have not checked to make sure that it is complete and correct.\n",
    "* __Do:__ Use AI for debugging and letting it explain code and concepts from class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "![GridHunt Image](https://mhahsler.github.io/CS7320-AI/Agents/gridhunt.png)\n",
    "\n",
    "Gridhunt is a small educational game that helps you practice implementing simple agent functions. It is a modified Python reimplementation of [Gridhunt2](https://michael.hahsler.net/SMU/CS1342/gridhunt2/). \n",
    "\n",
    "The objective of gridhunt is to implement an intelligent agent, the hunter, who can catch a monster faster than all other hunters. Gridhunt uses a $n \\times n$ arena consisting of a grid of tiles. Gridhunt is a turn-based game, and several hunter agents and the monster agent move around in the arena. A hunter catches the monster if she/he moves on the same square the monster currently occupies. The hunters compete fow who catches the monster first. If the monster survives the maximum number of steps for the game, then the monster wins.\n",
    "\n",
    "## PEAS Description of Gridhunt\n",
    "\n",
    "__Performance Measure:__ The performance is measured as the number of steps the hunter uses to catch the monster. Catching the monster means to be on the same square.\n",
    "\n",
    "__Environment:__ An arena with $n \\times n$ squares. At the beginning the monster and the hunters are randomly placed in the arena environment. The hunters and the monster\n",
    "    can move around, but cannot leave the arena. Since there are three agents involved, this is a competitive multi-agent environment.\n",
    "\n",
    "__Actuators:__ The agent can move to an adjacent square using actions \"north\", \"east\", \"west\", and \"south\", or teleport which will move the agent \n",
    "    to a random square in the arena.\n",
    "\n",
    "__Sensors:__ The agent can always see its location in the arena and the location of the monster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Arena Environment\n",
    "\n",
    "The environment manages the location of the agents (hunter and monster). It initially places them in a random location and then provides them in every step with percepts (the location of the hunter and the monster) and asks them for their action. \n",
    "\n",
    "__A Note on positions:__\n",
    "The arena is implemented as an array with row and column indices representing the position.\n",
    "Positions are stored in a numpy array where `pos[0]` is the row index and `pos[1]` is the column index in the arena. North means up in this array, that is the row index gets smaller when going north. Remember indices in Python start with 0 and go to `n-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def arena_environment(hunter_1_agent_function, hunter_2_agent_function, monster_agent_function, n = 30, max_steps = 100, visualize = False, animation = False):\n",
    "    \"\"\"\n",
    "    Simulate an arena where two hunter agent tries to catch a monster agent.\n",
    "\n",
    "    Parameters:\n",
    "    hunter_1_agent_function and hunter_2_agent_function (function): A function that takes the current positions of the hunter and monster\n",
    "                                      and returns the next move for the hunter ('north', 'east', 'west', 'south', 'teleport').\n",
    "    monster_agent_function (function): A function that takes the current positions of the hunter and monster\n",
    "                                       and returns the next move for the monster (or 'stay' to remain in place).\n",
    "    n (int): The size of the arena (n x n grid).\n",
    "    max_steps (int): The maximum number of steps to simulate.\n",
    "    visualize (bool): Whether to visualize the arena.\n",
    "    animation (bool): Whether to animate the visualization with a delay.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of steps taken for the hunter to catch the monster or np.nan (not a number) if not caught.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize positions\n",
    "    monster_pos = np.random.randint(0, n-1, size=2)\n",
    "    hunter_1_pos = np.random.randint(0, n-1, size=2)\n",
    "    while np.array_equal(hunter_1_pos, monster_pos):\n",
    "        hunter_1_pos = np.random.randint(0, n-1, size=2)\n",
    "    hunter_2_pos = np.random.randint(0, n-1, size=2)\n",
    "    while np.array_equal(hunter_2_pos, monster_pos):\n",
    "        hunter_2_pos = np.random.randint(0, n-1, size=2)    \n",
    "    \n",
    "    def move(action, position):\n",
    "        \"\"\"calculate new position for the agent.\"\"\"\n",
    "        if action == 'north':\n",
    "            position[0] -= 1\n",
    "        elif action == 'south':\n",
    "            position[0] += 1\n",
    "        elif action == 'west':\n",
    "            position[1] -= 1\n",
    "        elif action == 'east':\n",
    "            position[1] += 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "        \n",
    "        # Ensure position stays within bounds\n",
    "        position = np.clip(position, 0, n-1)\n",
    "        return position\n",
    "\n",
    "    def print_arena():\n",
    "\n",
    "        if animation:\n",
    "            sleep(1)\n",
    "            clear_output(wait=True)\n",
    "        print(f\"Step {step}:\")\n",
    "        print(f\"Hunter 1 is at '{hunter_1_pos}'; Hunter 2 is at '{hunter_2_pos}'; Monster is at '{monster_pos}'\\n\")\n",
    "        arena = np.full((n, n), '.', dtype=str)\n",
    "        arena[monster_pos[0], monster_pos[1]] = 'M'\n",
    "        arena[hunter_1_pos[0], hunter_1_pos[1]] = '1'\n",
    "        arena[hunter_2_pos[0], hunter_2_pos[1]] = '2'\n",
    "        print(\"\\n\".join(\"\".join(row) for row in arena))\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if visualize:\n",
    "            print_arena()\n",
    "        \n",
    "        # Check if hunter has caught the monster\n",
    "        if np.array_equal(hunter_1_pos, monster_pos):\n",
    "            if visualize:\n",
    "                print(f\"Hunter 1 caught the monster in {step} steps!\")\n",
    "            return (\"1\", step)\n",
    "        \n",
    "        if np.array_equal(hunter_2_pos, monster_pos):\n",
    "            if visualize:\n",
    "                print(f\"Hunter 2 caught the monster in {step} steps!\")\n",
    "            return (\"2\", step)\n",
    "\n",
    "        # Get next move from monster agent function\n",
    "        monster_action = monster_agent_function(monster_pos, hunter_1_pos, hunter_2_pos)\n",
    "        if monster_action != 'stay':\n",
    "            monster_pos = move(monster_action, monster_pos)\n",
    "\n",
    "        # Get next move from hunter agent function\n",
    "        hunter_1_action = hunter_1_agent_function(hunter_1_pos, monster_pos)\n",
    "        if hunter_1_action == 'teleport':\n",
    "            hunter_1_pos = np.random.randint(0, n-1, size=2)\n",
    "        else:\n",
    "            hunter_1_pos = move(hunter_1_action, hunter_1_pos)\n",
    "\n",
    "        hunter_2_action = hunter_2_agent_function(hunter_2_pos, monster_pos)\n",
    "        if hunter_2_action == 'teleport':\n",
    "            hunter_2_pos = np.random.randint(0, n-1, size=2)\n",
    "        else:\n",
    "            hunter_2_pos = move(hunter_2_action, hunter_2_pos)\n",
    "    \n",
    "        # print the agents' actions\n",
    "        if visualize:\n",
    "            print(f\"Hunter 1 chose action '{hunter_1_action}'\")\n",
    "            print(f\"Hunter 2 chose action '{hunter_2_action}'\")\n",
    "            print(f\"Monster chose action '{monster_action}'\")\n",
    "            print(\"\\n\" + \"=\"*n + \"\\n\")\n",
    "\n",
    "    # the hunter failed to catch the monster within max_steps\n",
    "    if visualize:\n",
    "        print(f\"Monster won by surviving {max_steps} steps.\")\n",
    "    \n",
    "    return (\"M\", np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implement the monster here as a simple agent that moves around randomly, but mostly stays in its place. You can use AI to get a detailed explanation of how the following code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "monster_actions = [\"north\", \"east\", \"west\", \"south\", \"stay\"]\n",
    "\n",
    "def monster_agent_function_simple(monster_pos, hunter_1_pos, hunter_2_pos):\n",
    "    return np.random.choice(monster_actions, p=[0.125, 0.125, 0.125, 0.125, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hunter Agent\n",
    "\n",
    "Your job is to implement a simple-reflex hunter agent that can catch the monster. Remember, your agent is implemented as an agent function that get percepts and needs to return a valid action. The actions are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"north\", \"east\", \"west\", \"south\", \"teleport\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example Implementation\n",
    "\n",
    "Here is a very simple hunter that just runs around randomly and hopes that it bumps into the monster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_randomized_hunter_agent_function(hunter_location, monster_location):\n",
    "    return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the agent for an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('north')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_randomized_hunter_agent_function([0,0], [5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting With the Agent\n",
    "\n",
    "We can place the monster and the hunter into the environment and run a simulation by calling the environment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "Hunter 1 is at '[2 1]'; Hunter 2 is at '[2 3]'; Monster is at '[3 2]'\n",
      "\n",
      ".....\n",
      ".....\n",
      ".1.2.\n",
      "..M..\n",
      ".....\n",
      "Hunter 1 chose action 'west'\n",
      "Hunter 2 chose action 'north'\n",
      "Monster chose action 'west'\n",
      "\n",
      "=====\n",
      "\n",
      "Step 1:\n",
      "Hunter 1 is at '[2 0]'; Hunter 2 is at '[1 3]'; Monster is at '[3 1]'\n",
      "\n",
      ".....\n",
      "...2.\n",
      "1....\n",
      ".M...\n",
      ".....\n",
      "Hunter 1 chose action 'teleport'\n",
      "Hunter 2 chose action 'west'\n",
      "Monster chose action 'north'\n",
      "\n",
      "=====\n",
      "\n",
      "Step 2:\n",
      "Hunter 1 is at '[2 1]'; Hunter 2 is at '[1 2]'; Monster is at '[2 1]'\n",
      "\n",
      ".....\n",
      "..2..\n",
      ".1...\n",
      ".....\n",
      ".....\n",
      "Hunter 1 caught the monster in 2 steps!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1', 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hunter_1 = simple_randomized_hunter_agent_function\n",
    "hunter_2 = simple_randomized_hunter_agent_function\n",
    "\n",
    "arena_environment(hunter_1, \n",
    "                  hunter_2, \n",
    "                  monster_agent_function_simple, \n",
    "                  n=5, max_steps=5, visualize=True, animation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this was one run, maybe it was just good or bad luck this time!\n",
    "\n",
    "Let's run an experiment with 100 runs in a $20 \\times 20$ arena.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2', 16), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('2', 11), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('1', 3), ('M', nan), ('2', 49), ('2', 94), ('2', 53), ('1', 19), ('1', 31), ('1', 91), ('M', nan), ('2', 8), ('M', nan), ('M', nan), ('1', 33), ('M', nan), ('M', nan), ('M', nan), ('1', 47), ('M', nan), ('M', nan), ('M', nan), ('1', 71), ('2', 33), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('1', 81), ('M', nan), ('M', nan), ('1', 7), ('M', nan), ('M', nan), ('2', 5), ('2', 57), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('1', 19), ('M', nan), ('M', nan), ('2', 1), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('2', 37), ('1', 69), ('M', nan), ('M', nan), ('M', nan), ('2', 80), ('2', 49), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('2', 86), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('M', nan), ('1', 46), ('M', nan), ('M', nan), ('2', 5), ('M', nan), ('1', 68), ('1', 10), ('1', 3), ('1', 97), ('2', 53), ('2', 21), ('2', 45), ('1', 63), ('2', 42)]\n"
     ]
    }
   ],
   "source": [
    "hunter_1 = simple_randomized_hunter_agent_function\n",
    "hunter_2 = simple_randomized_hunter_agent_function\n",
    "\n",
    "steps = [arena_environment(hunter_1, \n",
    "                           hunter_2, \n",
    "                           monster_agent_function_simple, \n",
    "                           n=20, max_steps=100, visualize=False) for _ in range(100)] \n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just got a list with who one and the number of steps to catch the monster for 100 simulation runs.\n",
    "Let's analysis the results by counting who won how often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hunter 1 won: 17\n",
      "Hunter 2 won: 19\n",
      "Monster 1 won: 64\n"
     ]
    }
   ],
   "source": [
    "steps = np.array(steps)\n",
    "\n",
    "print (f\"Hunter 1 won: {np.sum(steps[:,0] == \"1\")}\")\n",
    "print (f\"Hunter 2 won: {np.sum(steps[:,0] == \"2\")}\")\n",
    "print (f\"Monster 1 won: {np.sum(steps[:,0] == \"M\")}\")       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random agent in not a very good hunter. You need to implement a better agent function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Agent Implementation [4 points]\n",
    "\n",
    "Write a new hunter agent function that chases the monster. Copy the simple randomized hunter function from above and modify it using rules based on the monster's and the hunter's location. Let your hunter compete with the random agent. You can also come up with several different agents and let them compete against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here goes your agent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Experiments [2 points]\n",
    "\n",
    "Copy the simulation code from above and run experiments with your agent. \n",
    "Experiment with larger arenas of at least size $30 \\times 30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experimentation code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Conclusion [4 points] \n",
    "\n",
    "Discuss the following:\n",
    "\n",
    "* What is your hunter's final strategy to choose actions. Why does it work well?\n",
    "* Do you use teleportation. Why and in what situation? Why not?\n",
    "* How does the arena size affects your hunter agent's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your discussion goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monster is also an agent. Describe how the monster's agent function could be changed so it gets better at avoiding the hunter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your discussion goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Work (optional)\n",
    "\n",
    "* Improve the monster agent so it tries to run away from the hunters. Don;t make the monster too fast or the hunters will never be able to catch it.\n",
    "* How could the two hunter agents work together? You need to think about information sharing. This can be implemented by the two agent functions sharing a single Python class or the environment could be expanded by an action \"shout\" that lets the other agent perceive a message."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
