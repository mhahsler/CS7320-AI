{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents: Reflex-Based Agents for GridHunt\n",
    "\n",
    "Student Name: [Add your name]\n",
    "\n",
    "I have used the following AI tools: [list tools]\n",
    "\n",
    "I understand that my submission needs to be my own work: [your initials]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "* Apply core AI concepts by implementing the agent function for a simple and model-based reflex agents that respond to environmental percepts.\n",
    "* Practice how the environment and the agent function interact.\n",
    "* Analyze agent performance through controlled experiments across different environment configurations.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undergrads 10\n",
    "\n",
    "Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the completely rendered notebook.\n",
    "### AI Use\n",
    "\n",
    "Here are some guidelines that will make it easier for you:\n",
    "\n",
    "* __Don't:__ Rely on AI auto completion. You will waste a lot of time trying to figure out how the suggested code relates to what we do in class. Turn off AI code completion (e.g., Copilot) in your IDE.\n",
    "* __Don't:__ Do not submit code/text that you do not understand or have not checked to make sure that it is complete and correct.\n",
    "* __Do:__ Use AI for debugging and letting it explain code and concepts from class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "![GridHunt Image](https://mhahsler.github.io/CS7320-AI/Agents/gridhunt.png)\n",
    "\n",
    "Gridhunt is a small educational game that helps you practice implementing simple agent functions. It is a modified Python reimplementation of [Gridhunt2](https://michael.hahsler.net/SMU/CS1342/gridhunt2/). \n",
    "\n",
    "The objective of gridhunt is to implement an intelligent agent, the hunter, who can catch a monster faster than all other hunters. Gridhunt uses a $n \\times n$ arena consisting of a grid of tiles. Gridhunt is a turn-based game, and the hunter and the monster move on this grid. A hunter catches the monster if she/he moves on the same square the monster currently occupies. If the monster survives the maximum number of steps for the game, then the monster wins.\n",
    "\n",
    "## PEAS Description of Gridhunt\n",
    "\n",
    "__Performance Measure:__ The performance is measured as the number of steps the hunter uses to catch the monster. Catching the monster means to be on the same square.\n",
    "\n",
    "__Environment:__ An arena with $n \\times n$ squares. At the beginning the monster and the hunter are randomly placed in the arena environment. The hunter and the monster\n",
    "    can move around, but cannot leave the arena.\n",
    "\n",
    "__Actuators:__ The agent can move to an adjacent square using actions \"north\", \"east\", \"west\", and \"south\", or teleport which will move the agent \n",
    "    to a random square in the arena.\n",
    "\n",
    "__Sensors:__ The agent can always see its location in the arena and the location of the monster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Arena Environment\n",
    "\n",
    "The environment manages the location of the agents (hunter and monster). It initially places them in a random location and then provides them in every step with percepts (the location of the hunter and the monster) and asks them for their action. \n",
    "\n",
    "__A Note on positions:__\n",
    "The arena is implemented as an array with row and column indices representing the position.\n",
    "Positions are stored in a numpy array where `pos[0]` is the row index and `pos[1]` is the column index in the arena. North means up in this array, that is the row index gets smaller when going north. Remember indices in Python start with 0 and go to `n-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def arena_environment(hunter_agent_function, monster_agent_function, n = 30, max_steps = 100, visualize = False, animation = False):\n",
    "    \"\"\"\n",
    "    Simulate an arena where a hunter agent tries to catch a monster agent.\n",
    "\n",
    "    Parameters:\n",
    "    hunter_agent_function (function): A function that takes the current positions of the hunter and monster\n",
    "                                      and returns the next move for the hunter ('north', 'east', 'west', 'south', 'teleport').\n",
    "    monster_agent_function (function): A function that takes the current positions of the hunter and monster\n",
    "                                       and returns the next move for the monster (or 'stay' to remain in place).\n",
    "    n (int): The size of the arena (n x n grid).\n",
    "    max_steps (int): The maximum number of steps to simulate.\n",
    "    visualize (bool): Whether to visualize the arena.\n",
    "    animation (bool): Whether to animate the visualization with a delay.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of steps taken for the hunter to catch the monster or np.nan (not a number) if not caught.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize positions\n",
    "    monster_pos = np.random.randint(0, n-1, size=2)\n",
    "    hunter_pos = np.random.randint(0, n-1, size=2)\n",
    "    \n",
    "    def move(action, position):\n",
    "        \"\"\"calculate new position for the agent.\"\"\"\n",
    "        if action == 'north':\n",
    "            position[0] -= 1\n",
    "        elif action == 'south':\n",
    "            position[0] += 1\n",
    "        elif action == 'west':\n",
    "            position[1] -= 1\n",
    "        elif action == 'east':\n",
    "            position[1] += 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "        \n",
    "        # Ensure position stays within bounds\n",
    "        position = np.clip(position, 0, n-1)\n",
    "        return position\n",
    "\n",
    "    def print_arena():\n",
    "\n",
    "        if animation:\n",
    "            sleep(1)\n",
    "            clear_output(wait=True)\n",
    "        print(f\"Step {step}:\")\n",
    "        print(f\"Hunter is at '{hunter_pos}'; Monster is at '{monster_pos}'\\n\")\n",
    "        arena = np.full((n, n), '.', dtype=str)\n",
    "        arena[monster_pos[0], monster_pos[1]] = 'M'\n",
    "        arena[hunter_pos[0], hunter_pos[1]] = 'H'\n",
    "        print(\"\\n\".join(\"\".join(row) for row in arena))\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if visualize:\n",
    "            print_arena()\n",
    "        \n",
    "        # Check if hunter has caught the monster\n",
    "        if np.array_equal(hunter_pos, monster_pos):\n",
    "            if visualize:\n",
    "                print(f\"Hunter caught the monster in {step} steps!\")\n",
    "            return step\n",
    "        \n",
    "        # Get next move from monster agent function\n",
    "        monster_action = monster_agent_function(hunter_pos, monster_pos)\n",
    "        if monster_action != 'stay':\n",
    "            monster_pos = move(monster_action, monster_pos)\n",
    "\n",
    "        # Get next move from hunter agent function\n",
    "        hunter_action = hunter_agent_function(hunter_pos, monster_pos)\n",
    "        if hunter_action == 'teleport':\n",
    "            hunter_pos = np.random.randint(0, n-1, size=2)\n",
    "        else:\n",
    "            hunter_pos = move(hunter_action, hunter_pos)\n",
    "    \n",
    "        # print the agents' actions\n",
    "        if visualize:\n",
    "            print(f\"Hunter chose action '{hunter_action}'\")\n",
    "            print(f\"Monster chose action '{monster_action}'\")\n",
    "            print(\"\\n\" + \"=\"*n + \"\\n\")\n",
    "\n",
    "    # the hunter failed to catch the monster within max_steps\n",
    "    if visualize:\n",
    "        print(f\"Hunter failed to catch the monster in {max_steps} steps.\")\n",
    "    \n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implement the monster here as a simple agent that moves around randomly, but mostly stays in its place. You can use AI to get a detailed explanation of how the following code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "monster_actions = [\"north\", \"east\", \"west\", \"south\", \"stay\"]\n",
    "\n",
    "def monster_agent_function_simple(hunter_pos, monster_pos):\n",
    "    return np.random.choice(monster_actions, p=[0.125, 0.125, 0.125, 0.125, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hunter Agent\n",
    "\n",
    "Your job is to implement a simple-reflex hunter agent that can catch the monster. Remember, your agent is implemented as an agent function that get percepts and needs to return a valid action. The actions are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"north\", \"east\", \"west\", \"south\", \"teleport\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example Implementation\n",
    "\n",
    "Here is a very simple hunter that just runs around randomly and hopes that it bumps into the monster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_randomized_hunter_agent_function(hunter_location, monster_location):\n",
    "    return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the agent for an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('west')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_randomized_hunter_agent_function([0,0], [5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting With the Agent\n",
    "\n",
    "We can place the monster and the hunter into the environment and run a simulation by calling the environment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "Hunter is at '[0 2]'; Monster is at '[1 3]'\n",
      "\n",
      "..H..\n",
      "...M.\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "Hunter chose action 'east'\n",
      "Monster chose action 'stay'\n",
      "\n",
      "=====\n",
      "\n",
      "Step 1:\n",
      "Hunter is at '[0 3]'; Monster is at '[1 3]'\n",
      "\n",
      "...H.\n",
      "...M.\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "Hunter chose action 'west'\n",
      "Monster chose action 'north'\n",
      "\n",
      "=====\n",
      "\n",
      "Step 2:\n",
      "Hunter is at '[0 2]'; Monster is at '[0 3]'\n",
      "\n",
      "..HM.\n",
      ".....\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "Hunter chose action 'east'\n",
      "Monster chose action 'stay'\n",
      "\n",
      "=====\n",
      "\n",
      "Step 3:\n",
      "Hunter is at '[0 3]'; Monster is at '[0 3]'\n",
      "\n",
      "...H.\n",
      ".....\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "Hunter caught the monster in 3 steps!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arena_environment(simple_randomized_hunter_agent_function, monster_agent_function_simple, n=5, max_steps=5, visualize=True, animation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this was one run, maybe it was just good or bad luck this time!\n",
    "\n",
    "Let's run an experiment with 100 runs in a $10 \\times 10$ arena.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, nan, 54, nan, 28, nan, nan, nan, 67, nan, nan, nan, 44, 65, nan, nan, nan, nan, nan, 31, nan, nan, 54, 43, 63, 75, 30, 29, 81, 85, 1, 13, nan, nan, nan, 29, 55, 44, 21, nan, nan, 31, 73, 38, 53, 22, nan, nan, 72, 4, nan, nan, 53, 6, 97, nan, nan, 0, nan, nan, nan, 55, nan, nan, nan, nan, 65, nan, nan, 59, 26, nan, nan, 27, 93, 95, 4, 13, 21, 11, 74, 19, nan, 44, nan, 28, 45, nan, nan, 11, 25, nan, nan, 47, nan, nan, 40, nan, 32, 95]\n"
     ]
    }
   ],
   "source": [
    "steps = [arena_environment(simple_randomized_hunter_agent_function, monster_agent_function_simple, n=10, max_steps=100, visualize=False) for _ in range(100)] \n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just got a list with the number of steps to catch the monster for 100 simulation runs.\n",
    "Let's analysis the results. `nan` means that the hunter was not able to catch the monster. How many times did that happen? To answer how well the hunter did when it caught the monster, we just average the numbers that are not `nan`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hunter failed 46 times.\n",
      "Average steps to catch the monster over 54 successful runs: 43.46\n"
     ]
    }
   ],
   "source": [
    "print (f\"Hunter failed {np.sum(np.isnan(steps))} times.\")\n",
    "print (f\"Average steps to catch the monster over {np.sum(~np.isnan(steps))} successful runs: {round(np.nanmean(steps), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This in not a very good agent. You need to implement a better agent function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Agent Implementation [4 points]\n",
    "\n",
    "Write a new hunter agent function that chases the monster. Copy the simple randomized hunter function from above and modify it using rules based on the monster's and the hunter's location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here goes your agent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Experiments [2 points]\n",
    "\n",
    "Copy the simulation code from above and run experiments with your agent. \n",
    "Experiment with larger arenas of at least size $30 \\times 30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experimentation code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Conclusion [4 points] \n",
    "\n",
    "Discuss the following:\n",
    "\n",
    "* What is your hunter's final strategy to choose actions. Why does it work well?\n",
    "* Do you use teleportation. Why and in what situation? Why not?\n",
    "* How does the arena size affects your hunter agent's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your discussion goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monster is also an agent. Describe how the monster's agent function could be changed so it gets better at avoiding the hunter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your discussion goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Work (Optional)\n",
    "\n",
    "Here are some ideas:\n",
    "\n",
    "* Implement a better monster agent function and perform experiments\n",
    "* Change the environment so multiple hunters can hunt the monster.\n",
    "* Give the hunter a new action that shoots an arrow in a specific direction. The arrow can go a maximum of 5 squares. \n",
    "  If the hunter hits the monster, then it wins.\n",
    "* Change the environment so that the hunter does not receive the monster position. It has to stop and use a new action \"look\" and then the \n",
    "  environment gives it the location the next time. You probably need to implement a model-based reflex agent that can remember the location \n",
    "  where it has seen the agent last: [Help with implementing state information in Python](https://github.com/mhahsler/CS7320-AI/blob/master/HOWTOs/store_agent_state_information.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; 2025 [Michael Hahsler](http://michael.hahsler.net). \n",
    "This work is openly licensed under [Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "![CC BY-SA 4.0](https://licensebuttons.net/l/by-sa/3.0/88x31.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
