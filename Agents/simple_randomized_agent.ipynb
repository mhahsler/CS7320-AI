{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents: Vacuum-cleaner World\n",
    "\n",
    "Implement a simulator environment for a vacuum-cleaner world and a set of intelligent agents.\n",
    "\n",
    "## PEAS description\n",
    "\n",
    "__Performance Measure:__ Each action costs 1. The performance is measured as the sum of the cost to clean the whole environment.\n",
    "\n",
    "__Environment:__ A room with $n \\times n$ squares where $n = 5$. Dirt is randomly placed on each square with probability $p = 0.2$. For simplicity, you can assume that the agent knows the size of the layout of the room (i.e., it knows n and where it starts).\n",
    "\n",
    "__Actuators:__ The agent can `clean` the current square or move to an adjacent square by going `north`, `east`, `west`, or `south`.\n",
    "\n",
    "__Sensors:__ Four bumper sensors, one for north`, `east`, `west`, and `south`; a dirt sensor reporting dirt in the current square.  \n",
    "\n",
    "The easiest implementation for the environment is to hold an 2-dimensional array to represent if squares are clean or dirty and to call the agent function in a loop untill all squares are clean or a predefined number of steps have been reached.\n",
    "\n",
    "## Define the agent program for a simple randomized agent\n",
    "\n",
    "The agent program is a function that gets sensor information (the current percepts) as the arguments. The arguments are:\n",
    "\n",
    "* A dictonary with boolean entries for the for bumper sensors `north`, `east`, `west`, `south`; not specified bumpers are assumed to be `False`. E.g., if the agent is on the north-west corner, `bumpers` gets `{\"north\" : True, \"east\" : True}` or if the agent is not close to a border then it gets `{}`.\n",
    "* The dirt sensor produces a boolean.\n",
    "\n",
    "The agent returns the chosen action as a string.\n",
    "\n",
    "Here is an example implementation for the agent program of a simple randomized agent:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "\n",
    "\n",
    "def simple_randomized_agent(bumpers, dirty):\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suck'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_randomized_agent({\"north\" : True}, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the agent 10 times steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "south\n",
      "west\n",
      "east\n",
      "west\n",
      "east\n",
      "suck\n",
      "suck\n",
      "west\n",
      "north\n",
      "north\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(simple_randomized_agent({}, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "\n",
    "## Task 1: Implement a simulation environment\n",
    "\n",
    "Your environment simulator needs to create squares, make some dirty, and proivde the agent function with the sensor inputs. The environment needs to evaluate the performance measure. It needs to track the agent's actions until all dirty squares are clean and count the number of actions it takes the agent to complete the task.\n",
    "\n",
    "The simulation environment needs to work with the simple randomized agent program from above.\n",
    "\n",
    "\n",
    "## Task 2:  Implement a simple reflex agent\n",
    "\n",
    "The simple reflex agent randomly walks around but reacts to the bumper sensor by not bumping into the wall and to dirt with sucking.\n",
    "\n",
    "## Task 3: Implement a model-based reflex agent \n",
    "\n",
    "This agent keeps track of the location and remembers where it has cleaned. It can use more advanced navigation.\n",
    "\n",
    "## Task 4: Simulation study\n",
    "\n",
    "Compare the performance of the agents using different size environments. E.g., $5 \\times 5$, $10 \\times 10$ and\n",
    "$100 \\times 100$. Use at least 100 random runs for each.\n",
    "\n",
    "## Bonus tasks\n",
    "\n",
    "* __Obstacles:__ Add random obstacle squares that also trigger the bumper sensor. The agent does not know where the obstacles are. How does this change the performance?\n",
    "* __Unknown environment with obstacles:__ The agent does not know how large the environment is, where it starts or where the obstacles are.\n",
    "* __Utility-based agent:__ Change the environment, so each square has a fixed probability of getting dirty again. Give this information to the agent (as a 2-dimensional array of probabilities). Cleaning one dirty square produces the utility of 1. Implement a utility-based agent that maximizes the expected utility over a time horizon of 10000 time steps. This is very tricky!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
